{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16797832",
   "metadata": {},
   "source": [
    "# ADX File-Transfer Analytics Runbook\n",
    "\n",
    "Interactive notebook for setting up, ingesting data, and verifying the ADX file-transfer analytics pipeline.  \n",
    "Uses the same schema, mappings, and update policy as the production Event Grid pipeline (FR-034).\n",
    "\n",
    "**Workflow:**\n",
    "1. **Configure** — Set cluster URI, database, and auth method\n",
    "2. **Setup** — Create all ADX objects (tables, mappings, policies, materialized view)\n",
    "3. **Ingest** — Load CSV/JSON data from local files or Azure Blob Storage\n",
    "4. **Verify** — Query the target table to confirm data landed correctly\n",
    "\n",
    "> **Tip:** The CLI script `adx_runbook.py` is also available for scripted/CI usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590851f7",
   "metadata": {},
   "source": [
    "## 1. Prerequisites & Imports\n",
    "\n",
    "Ensure you have run `uv pip install -r requirements.txt` in this directory's venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SSL fix: uv-managed Python may ship without CA certs. If SSL_CERT_FILE is\n",
    "# not set, point it at certifi's bundle so TLS connections work out of the box.\n",
    "# ---------------------------------------------------------------------------\n",
    "if not os.environ.get(\"SSL_CERT_FILE\"):\n",
    "    try:\n",
    "        import certifi\n",
    "        os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "        print(f\"SSL_CERT_FILE set to {certifi.where()}\")\n",
    "    except ImportError:\n",
    "        print(\"WARNING: certifi not installed — TLS may fail if system CA certs are missing\")\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.kusto.data import KustoClient, KustoConnectionStringBuilder\n",
    "from azure.kusto.data.data_format import DataFormat\n",
    "from azure.kusto.data.exceptions import KustoServiceError\n",
    "from azure.kusto.ingest import (\n",
    "    QueuedIngestClient,\n",
    "    IngestionProperties,\n",
    "    FileDescriptor,\n",
    "    BlobDescriptor,\n",
    ")\n",
    "\n",
    "print(\"All imports OK ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe26dbd",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your ADX cluster URI and database name. Change the auth method if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9641398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cluster & Database ───────────────────────────────────────────────────────\n",
    "CLUSTER_URI = \"https://adx-ft-dev.eastus2.kusto.windows.net\"\n",
    "INGEST_URI  = \"https://ingest-adx-ft-dev.eastus2.kusto.windows.net\"\n",
    "DATABASE    = \"ftevents_dev\"\n",
    "\n",
    "# ── Authentication ────────────────────────────────────────────────────────────\n",
    "# Options: \"az-cli\" | \"interactive\" | \"managed-identity\" | \"service-principal\"\n",
    "AUTH_METHOD = \"az-cli\"\n",
    "\n",
    "# Only needed for service-principal auth:\n",
    "SP_CLIENT_ID     = os.environ.get(\"AZURE_CLIENT_ID\", \"\")\n",
    "SP_CLIENT_SECRET = os.environ.get(\"AZURE_CLIENT_SECRET\", \"\")\n",
    "SP_TENANT_ID     = os.environ.get(\"AZURE_TENANT_ID\", \"\")\n",
    "\n",
    "print(f\"Cluster:  {CLUSTER_URI}\")\n",
    "print(f\"Ingest:   {INGEST_URI}\")\n",
    "print(f\"Database: {DATABASE}\")\n",
    "print(f\"Auth:     {AUTH_METHOD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990834b1",
   "metadata": {},
   "source": [
    "## 3. Authentication Helper\n",
    "\n",
    "Builds a `KustoConnectionStringBuilder` for the chosen auth method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ec0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kcsb(cluster_uri: str) -> KustoConnectionStringBuilder:\n",
    "    \"\"\"Build a KustoConnectionStringBuilder based on AUTH_METHOD.\"\"\"\n",
    "    if AUTH_METHOD == \"az-cli\":\n",
    "        credential = DefaultAzureCredential(\n",
    "            exclude_interactive_browser_credential=True,\n",
    "            exclude_shared_token_cache_credential=True,\n",
    "        )\n",
    "        return KustoConnectionStringBuilder.with_azure_token_credential(cluster_uri, credential)\n",
    "    elif AUTH_METHOD == \"interactive\":\n",
    "        return KustoConnectionStringBuilder.with_interactive_login(cluster_uri)\n",
    "    elif AUTH_METHOD == \"managed-identity\":\n",
    "        return KustoConnectionStringBuilder.with_aad_managed_service_identity_authentication(cluster_uri)\n",
    "    elif AUTH_METHOD == \"service-principal\":\n",
    "        assert all([SP_CLIENT_ID, SP_CLIENT_SECRET, SP_TENANT_ID]), (\n",
    "            \"Set SP_CLIENT_ID, SP_CLIENT_SECRET, SP_TENANT_ID for service-principal auth\"\n",
    "        )\n",
    "        return KustoConnectionStringBuilder.with_aad_application_key_authentication(\n",
    "            cluster_uri, SP_CLIENT_ID, SP_CLIENT_SECRET, SP_TENANT_ID\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown auth method: {AUTH_METHOD}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Retry helper for transient network errors\n",
    "# ---------------------------------------------------------------------------\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "\n",
    "def execute_with_retry(\n",
    "    client: KustoClient, database: str, command: str, *, retries: int = MAX_RETRIES\n",
    ") -> object:\n",
    "    \"\"\"Execute a management command with retries for transient network errors.\"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return client.execute_mgmt(database, command)\n",
    "        except KustoServiceError as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"failed to process network request\" in error_msg or \"auth/metadata\" in error_msg:\n",
    "                last_exc = e\n",
    "                if attempt < retries:\n",
    "                    print(f\"  RETRY ({attempt}/{retries}, waiting {RETRY_DELAY}s)...\", flush=True)\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                    continue\n",
    "            raise\n",
    "    raise last_exc  # type: ignore[misc]\n",
    "\n",
    "\n",
    "print(\"Helpers defined ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78117d",
   "metadata": {},
   "source": [
    "## 4. Schema Commands\n",
    "\n",
    "The full DDL chain — identical to `kql/schema/*.kql` files. All commands are idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_COMMANDS = [\n",
    "    # Step 1: Target table\n",
    "    (\n",
    "        \"Create target table (FileTransferEvents)\",\n",
    "        \"\"\"\\\n",
    ".create-merge table FileTransferEvents (\n",
    "    Filename: string,\n",
    "    SourcePresent: bool,\n",
    "    TargetPresent: bool,\n",
    "    SourceLastModifiedUtc: datetime,\n",
    "    TargetLastModifiedUtc: datetime,\n",
    "    AgeMinutes: real,\n",
    "    Status: string,\n",
    "    Notes: string,\n",
    "    Timestamp: datetime\n",
    ")\"\"\",\n",
    "    ),\n",
    "    # Step 2: Staging table\n",
    "    (\n",
    "        \"Create staging table (FileTransferEvents_Raw)\",\n",
    "        \"\"\"\\\n",
    ".create-merge table FileTransferEvents_Raw (\n",
    "    Filename: string,\n",
    "    SourcePresent: bool,\n",
    "    TargetPresent: bool,\n",
    "    SourceLastModifiedUtc: datetime,\n",
    "    TargetLastModifiedUtc: datetime,\n",
    "    AgeMinutes: real,\n",
    "    Status: string,\n",
    "    Notes: string\n",
    ")\"\"\",\n",
    "    ),\n",
    "    # Step 3: Dead-letter table\n",
    "    (\n",
    "        \"Create dead-letter table (FileTransferEvents_Errors)\",\n",
    "        \"\"\"\\\n",
    ".create-merge table FileTransferEvents_Errors (\n",
    "    RawData: string,\n",
    "    Database: string,\n",
    "    ['Table']: string,\n",
    "    FailedOn: datetime,\n",
    "    Error: string,\n",
    "    OperationId: guid\n",
    ")\"\"\",\n",
    "    ),\n",
    "    # Step 4: Transformation function\n",
    "    (\n",
    "        \"Create transformation function\",\n",
    "        \"\"\"\\\n",
    ".create-or-alter function FileTransferEvents_Transform() {\n",
    "    FileTransferEvents_Raw\n",
    "    | extend Timestamp = coalesce(SourceLastModifiedUtc, ingestion_time())\n",
    "    | project Filename, SourcePresent, TargetPresent,\n",
    "              SourceLastModifiedUtc, TargetLastModifiedUtc,\n",
    "              AgeMinutes, Status, Notes, Timestamp\n",
    "}\"\"\",\n",
    "    ),\n",
    "    # Step 5: Update policy\n",
    "    (\n",
    "        \"Attach update policy\",\n",
    "        \".alter table FileTransferEvents policy update \"\n",
    "        \"@'[{\\\"IsEnabled\\\": true, \\\"Source\\\": \\\"FileTransferEvents_Raw\\\", \"\n",
    "        \"\\\"Query\\\": \\\"FileTransferEvents_Transform()\\\", \"\n",
    "        \"\\\"IsTransactional\\\": true, \\\"PropagateIngestionProperties\\\": true}]'\",\n",
    "    ),\n",
    "    # Step 6: CSV mapping (single-line body — execute_mgmt requires it)\n",
    "    (\n",
    "        \"Create CSV ingestion mapping\",\n",
    "        \".create-or-alter table FileTransferEvents_Raw ingestion csv mapping 'FileTransferEvents_CsvMapping' \"\n",
    "        \"'[\"\n",
    "        '{\"Name\":\"Filename\",\"DataType\":\"string\",\"Ordinal\":0},'\n",
    "        '{\"Name\":\"SourcePresent\",\"DataType\":\"bool\",\"Ordinal\":1},'\n",
    "        '{\"Name\":\"TargetPresent\",\"DataType\":\"bool\",\"Ordinal\":2},'\n",
    "        '{\"Name\":\"SourceLastModifiedUtc\",\"DataType\":\"datetime\",\"Ordinal\":3},'\n",
    "        '{\"Name\":\"TargetLastModifiedUtc\",\"DataType\":\"datetime\",\"Ordinal\":4},'\n",
    "        '{\"Name\":\"AgeMinutes\",\"DataType\":\"real\",\"Ordinal\":5},'\n",
    "        '{\"Name\":\"Status\",\"DataType\":\"string\",\"Ordinal\":6},'\n",
    "        '{\"Name\":\"Notes\",\"DataType\":\"string\",\"Ordinal\":7}'\n",
    "        \"]'\",\n",
    "    ),\n",
    "    # Step 7: JSON mapping (single-line body — execute_mgmt requires it)\n",
    "    (\n",
    "        \"Create JSON ingestion mapping\",\n",
    "        \".create-or-alter table FileTransferEvents_Raw ingestion json mapping 'FileTransferEvents_JsonMapping' \"\n",
    "        \"'[\"\n",
    "        '{\"column\":\"Filename\",\"path\":\"$.Filename\",\"datatype\":\"string\"},'\n",
    "        '{\"column\":\"SourcePresent\",\"path\":\"$.SourcePresent\",\"datatype\":\"bool\"},'\n",
    "        '{\"column\":\"TargetPresent\",\"path\":\"$.TargetPresent\",\"datatype\":\"bool\"},'\n",
    "        '{\"column\":\"SourceLastModifiedUtc\",\"path\":\"$.SourceLastModifiedUtc\",\"datatype\":\"datetime\"},'\n",
    "        '{\"column\":\"TargetLastModifiedUtc\",\"path\":\"$.TargetLastModifiedUtc\",\"datatype\":\"datetime\"},'\n",
    "        '{\"column\":\"AgeMinutes\",\"path\":\"$.AgeMinutes\",\"datatype\":\"real\"},'\n",
    "        '{\"column\":\"Status\",\"path\":\"$.Status\",\"datatype\":\"string\"},'\n",
    "        '{\"column\":\"Notes\",\"path\":\"$.Notes\",\"datatype\":\"string\"}'\n",
    "        \"]'\",\n",
    "    ),\n",
    "    # Step 8: Target table retention (90 days)\n",
    "    (\n",
    "        \"Set target table retention (90 days)\",\n",
    "        \".alter table FileTransferEvents policy retention \"\n",
    "        \"@'{\\\"SoftDeletePeriod\\\": \\\"90.00:00:00\\\", \\\"Recoverability\\\": \\\"Enabled\\\"}'\",\n",
    "    ),\n",
    "    # Step 9: Staging table retention (1 day)\n",
    "    (\n",
    "        \"Set staging table retention (1 day)\",\n",
    "        \".alter table FileTransferEvents_Raw policy retention \"\n",
    "        \"@'{\\\"SoftDeletePeriod\\\": \\\"1.00:00:00\\\", \\\"Recoverability\\\": \\\"Disabled\\\"}'\",\n",
    "    ),\n",
    "    # Step 10: Dead-letter retention (30 days)\n",
    "    (\n",
    "        \"Set dead-letter table retention (30 days)\",\n",
    "        \".alter table FileTransferEvents_Errors policy retention \"\n",
    "        \"@'{\\\"SoftDeletePeriod\\\": \\\"30.00:00:00\\\", \\\"Recoverability\\\": \\\"Disabled\\\"}'\",\n",
    "    ),\n",
    "    # Step 11: Ingestion batching (1 minute)\n",
    "    (\n",
    "        \"Set ingestion batching policy (1 min)\",\n",
    "        \".alter table FileTransferEvents_Raw policy ingestionbatching \"\n",
    "        \"@'{\\\"MaximumBatchingTimeSpan\\\": \\\"00:01:00\\\", \\\"MaximumNumberOfItems\\\": 20, \\\"MaximumRawDataSizeMB\\\": 256}'\",\n",
    "    ),\n",
    "    # Step 12: Materialized view\n",
    "    (\n",
    "        \"Create DailySummary materialized view\",\n",
    "        \"\"\"\\\n",
    ".create ifnotexists materialized-view DailySummary on table FileTransferEvents {\n",
    "    FileTransferEvents\n",
    "    | summarize\n",
    "        TotalCount      = count(),\n",
    "        OkCount         = countif(Status == \"OK\"),\n",
    "        MissingCount    = countif(Status == \"MISSING\"),\n",
    "        DelayedCount    = countif(Status == \"DELAYED\"),\n",
    "        AvgAgeMinutes   = avg(AgeMinutes),\n",
    "        AgeDigest       = tdigest(AgeMinutes)\n",
    "    by Date = startofday(Timestamp)\n",
    "}\"\"\",\n",
    "    ),\n",
    "    # Step 13: Materialized view retention (730 days)\n",
    "    (\n",
    "        \"Set DailySummary retention (730 days)\",\n",
    "        \".alter materialized-view DailySummary policy retention \"\n",
    "        \"@'{\\\"SoftDeletePeriod\\\": \\\"730.00:00:00\\\", \\\"Recoverability\\\": \\\"Enabled\\\"}'\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(SCHEMA_COMMANDS)} schema commands ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8bda82",
   "metadata": {},
   "source": [
    "## 5. Setup — Create ADX Schema\n",
    "\n",
    "Runs all 13 schema commands in order. All commands are idempotent — safe to re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98549e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcsb = build_kcsb(CLUSTER_URI)\n",
    "client = KustoClient(kcsb)\n",
    "\n",
    "print(f\"Setting up ADX schema in {DATABASE}...\")\n",
    "print(f\"  Cluster: {CLUSTER_URI}\")\n",
    "print()\n",
    "\n",
    "for i, (description, command) in enumerate(SCHEMA_COMMANDS, start=1):\n",
    "    step_label = f\"[{i:2d}/{len(SCHEMA_COMMANDS)}]\"\n",
    "    print(f\"  {step_label} {description}...\", end=\" \", flush=True)\n",
    "    try:\n",
    "        execute_with_retry(client, DATABASE, command)\n",
    "        print(\"OK\")\n",
    "    except KustoServiceError as e:\n",
    "        error_msg = str(e)\n",
    "        if \"already exists\" in error_msg.lower():\n",
    "            print(\"SKIPPED (already exists)\")\n",
    "        else:\n",
    "            print(f\"FAILED\\n         {error_msg}\")\n",
    "            raise\n",
    "\n",
    "print()\n",
    "print(\"Setup complete ✓  All tables, mappings, policies, and views are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa63c2c",
   "metadata": {},
   "source": [
    "## 6. Ingest Data\n",
    "\n",
    "Choose **one** of the options below:\n",
    "- **6a.** Ingest a local CSV/JSON file\n",
    "- **6b.** Ingest from Azure Blob Storage\n",
    "\n",
    "> **⚠ Note:** `QueuedIngestClient` is **fire-and-forget** — `ingest_from_file()` and `ingest_from_blob()` queue the request and return immediately. A \"queued ✓\" message does **not** mean data landed successfully. Run **Step 7** (Verify Ingestion) after 1–3 minutes to check for ingestion failures and confirm data arrived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268a860",
   "metadata": {},
   "source": [
    "### 6a. Ingest Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2738ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configure the file to ingest ─────────────────────────────────────────────\n",
    "LOCAL_FILE = \"../samples/sample-events.csv\"   # Relative to this notebook\n",
    "# LOCAL_FILE = \"../samples/sample-events.json\"\n",
    "\n",
    "# ── Auto-detect format & mapping from extension ──────────────────────────────\n",
    "file_path = Path(LOCAL_FILE).resolve()\n",
    "assert file_path.exists(), f\"File not found: {file_path}\"\n",
    "\n",
    "ext = file_path.suffix.lower()\n",
    "if ext == \".csv\":\n",
    "    data_format = DataFormat.CSV\n",
    "    mapping_name = \"FileTransferEvents_CsvMapping\"\n",
    "elif ext in (\".json\", \".jsonl\"):\n",
    "    data_format = DataFormat.JSON\n",
    "    mapping_name = \"FileTransferEvents_JsonMapping\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown extension '{ext}' — set data_format and mapping_name manually\")\n",
    "\n",
    "print(f\"File:    {file_path}\")\n",
    "print(f\"Size:    {file_path.stat().st_size:,} bytes\")\n",
    "print(f\"Format:  {data_format.name}\")\n",
    "print(f\"Mapping: {mapping_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bfdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_kcsb = build_kcsb(INGEST_URI)\n",
    "ingest_client = QueuedIngestClient(ingest_kcsb)\n",
    "\n",
    "ingestion_props = IngestionProperties(\n",
    "    database=DATABASE,\n",
    "    table=\"FileTransferEvents_Raw\",\n",
    "    data_format=data_format,\n",
    "    ingestion_mapping_reference=mapping_name,\n",
    "    ignore_first_record=(data_format == DataFormat.CSV),\n",
    ")\n",
    "\n",
    "print(f\"Ingesting {file_path.name} into FileTransferEvents_Raw...\")\n",
    "file_descriptor = FileDescriptor(str(file_path), file_path.stat().st_size)\n",
    "ingest_client.ingest_from_file(file_descriptor, ingestion_properties=ingestion_props)\n",
    "\n",
    "print()\n",
    "print(\"Ingestion queued ✓\")\n",
    "print(\"Data flows: staging table → update policy → FileTransferEvents\")\n",
    "print(\"Allow 1-3 minutes for rows to appear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04667a87",
   "metadata": {},
   "source": [
    "### 6b. Ingest from Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configure the blob URI ───────────────────────────────────────────────────\n",
    "BLOB_URI = \"https://stfteventsdev.blob.core.windows.net/file-transfer-events/sample-events.csv\"\n",
    "\n",
    "# ── Auto-detect format from URI extension ────────────────────────────────────\n",
    "path_part = BLOB_URI.split(\"?\")[0]\n",
    "if path_part.endswith(\".csv\"):\n",
    "    blob_format = DataFormat.CSV\n",
    "    blob_mapping = \"FileTransferEvents_CsvMapping\"\n",
    "elif path_part.endswith(\".json\") or path_part.endswith(\".jsonl\"):\n",
    "    blob_format = DataFormat.JSON\n",
    "    blob_mapping = \"FileTransferEvents_JsonMapping\"\n",
    "else:\n",
    "    raise ValueError(\"Cannot detect format from blob URI — set blob_format and blob_mapping manually\")\n",
    "\n",
    "ingest_kcsb = build_kcsb(INGEST_URI)\n",
    "ingest_client = QueuedIngestClient(ingest_kcsb)\n",
    "\n",
    "blob_props = IngestionProperties(\n",
    "    database=DATABASE,\n",
    "    table=\"FileTransferEvents_Raw\",\n",
    "    data_format=blob_format,\n",
    "    ingestion_mapping_reference=blob_mapping,\n",
    "    ignore_first_record=(blob_format == DataFormat.CSV),\n",
    ")\n",
    "\n",
    "print(f\"Ingesting blob into FileTransferEvents_Raw...\")\n",
    "print(f\"  Blob: {BLOB_URI}\")\n",
    "blob_descriptor = BlobDescriptor(BLOB_URI)\n",
    "ingest_client.ingest_from_blob(blob_descriptor, ingestion_properties=blob_props)\n",
    "\n",
    "print()\n",
    "print(\"Ingestion queued ✓\")\n",
    "print(\"Allow 1-3 minutes for rows to appear in FileTransferEvents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb60a5b",
   "metadata": {},
   "source": [
    "## 7. Verify Ingestion\n",
    "\n",
    "Checks for **ingestion failures** first (`.show ingestion failures`), then queries the target table and displays the most recent rows. Validates that `Timestamp` is non-null.\n",
    "\n",
    "> Run this cell 1–3 minutes after ingesting. If no rows appear but no failures either, wait and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_kcsb = build_kcsb(CLUSTER_URI)\n",
    "verify_client = KustoClient(verify_kcsb)\n",
    "\n",
    "# ── Step 1: Check for recent ingestion failures ──────────────────────────────\n",
    "FAILURE_QUERY = \".show ingestion failures | where FailedOn > ago(10m) | order by FailedOn desc | take 10\"\n",
    "\n",
    "print(f\"Checking ingestion failures in {DATABASE}...\")\n",
    "try:\n",
    "    fail_response = verify_client.execute_mgmt(DATABASE, FAILURE_QUERY)\n",
    "    fail_rows = list(fail_response.primary_results[0])\n",
    "    if fail_rows:\n",
    "        print(f\"\\n  ⚠ INGESTION FAILURES DETECTED ({len(fail_rows)} recent):\\n\")\n",
    "        for frow in fail_rows:\n",
    "            failed_on = frow[\"FailedOn\"] if \"FailedOn\" in frow.columns else \"?\"\n",
    "            details   = str(frow[\"Details\"])[:120] if \"Details\" in frow.columns else \"?\"\n",
    "            status    = frow[\"Status\"] if \"Status\" in frow.columns else \"?\"\n",
    "            print(f\"    [{status}] {failed_on}\")\n",
    "            print(f\"      {details}\")\n",
    "            print()\n",
    "        print(\"  Troubleshooting: Check RBAC — the ADX cluster managed identity needs\")\n",
    "        print(\"  'Storage Blob Data Reader' and 'Storage Blob Data Contributor' on the storage account.\\n\")\n",
    "    else:\n",
    "        print(\"  No ingestion failures in the last 10 minutes ✓\\n\")\n",
    "except KustoServiceError as e:\n",
    "    print(f\"  Could not check ingestion failures: {e}\\n\")\n",
    "\n",
    "# ── Step 2: Query the target table ───────────────────────────────────────────\n",
    "VERIFY_QUERY = \"\"\"\\\n",
    "FileTransferEvents\n",
    "| order by Timestamp desc\n",
    "| take 20\n",
    "| project Filename, SourcePresent, TargetPresent,\n",
    "          SourceLastModifiedUtc, TargetLastModifiedUtc,\n",
    "          AgeMinutes, Status, Notes, Timestamp\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Querying {DATABASE}.FileTransferEvents...\")\n",
    "response = verify_client.execute(DATABASE, VERIFY_QUERY)\n",
    "\n",
    "columns = [col.column_name for col in response.primary_results[0].columns]\n",
    "rows = list(response.primary_results[0])\n",
    "\n",
    "if not rows:\n",
    "    print(\"\\n  No rows found. If you just ingested, wait 1-3 minutes and re-run this cell.\")\n",
    "else:\n",
    "    print(f\"\\n  Found {len(rows)} recent rows:\\n\")\n",
    "    # Print as a simple table\n",
    "    header = \" | \".join(f\"{col:>20s}\" for col in columns)\n",
    "    print(f\"  {header}\")\n",
    "    print(f\"  {'-' * len(header)}\")\n",
    "    for row in rows:\n",
    "        vals = \" | \".join(f\"{str(row[col]):>20s}\" for col in columns)\n",
    "        print(f\"  {vals}\")\n",
    "\n",
    "    # Validate Timestamp\n",
    "    null_ts = sum(1 for r in rows if r[\"Timestamp\"] is None)\n",
    "    print()\n",
    "    if null_ts:\n",
    "        print(f\"  ⚠ WARNING: {null_ts} row(s) have null Timestamp!\")\n",
    "    else:\n",
    "        print(\"  All rows have non-null Timestamp ✓\")\n",
    "\n",
    "    # Status distribution\n",
    "    status_counts = {}\n",
    "    for r in rows:\n",
    "        s = str(r[\"Status\"])\n",
    "        status_counts[s] = status_counts.get(s, 0) + 1\n",
    "    print(f\"  Status distribution: {status_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350b4eb",
   "metadata": {},
   "source": [
    "## 8. Ad-Hoc Queries\n",
    "\n",
    "Run any KQL query against the database. Edit the query below and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a79964",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\\\n",
    "// Daily summary — SLA adherence computed at query time\n",
    "materialized_view(\"DailySummary\")\n",
    "| extend SlaAdherencePct = round(100.0 * OkCount / TotalCount, 2),\n",
    "         P95AgeMinutes   = percentile_tdigest(AgeDigest, 95)\n",
    "| project Date, TotalCount, OkCount, MissingCount, DelayedCount,\n",
    "          AvgAgeMinutes, P95AgeMinutes, SlaAdherencePct\n",
    "| order by Date desc\n",
    "| take 30\n",
    "\"\"\"\n",
    "\n",
    "adhoc_kcsb = build_kcsb(CLUSTER_URI)\n",
    "adhoc_client = KustoClient(adhoc_kcsb)\n",
    "\n",
    "result = adhoc_client.execute(DATABASE, QUERY)\n",
    "cols = [c.column_name for c in result.primary_results[0].columns]\n",
    "rows = list(result.primary_results[0])\n",
    "\n",
    "print(f\"Returned {len(rows)} rows\\n\")\n",
    "header = \" | \".join(f\"{c:>18s}\" for c in cols)\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for row in rows:\n",
    "    print(\" | \".join(f\"{str(row[c]):>18s}\" for c in cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453a977",
   "metadata": {},
   "source": [
    "## 9. Check Dead-Letter Table\n",
    "\n",
    "Shows recent ingestion failures. Useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_QUERY = \"\"\"\\\n",
    "FileTransferEvents_Errors\n",
    "| order by FailedOn desc\n",
    "| take 10\n",
    "| project FailedOn, Error, RawData, OperationId\n",
    "\"\"\"\n",
    "\n",
    "dl_kcsb = build_kcsb(CLUSTER_URI)\n",
    "dl_client = KustoClient(dl_kcsb)\n",
    "\n",
    "dl_result = dl_client.execute(DATABASE, DL_QUERY)\n",
    "dl_rows = list(dl_result.primary_results[0])\n",
    "\n",
    "if not dl_rows:\n",
    "    print(\"No dead-letter rows ✓\")\n",
    "else:\n",
    "    print(f\"Found {len(dl_rows)} error rows:\\n\")\n",
    "    for row in dl_rows:\n",
    "        print(f\"  {row['FailedOn']}  |  {row['Error'][:80]}\")\n",
    "        print(f\"    RawData: {str(row['RawData'])[:120]}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.8.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
